{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Poisoned Goat Experiment Pipeline\n",
        "\n",
        "This notebook orchestrates the full experiment:\n",
        "1. Generate contaminated addition datasets with different contamination rates (10%, 50%, 100%)\n",
        "2. Fine-tune tiedong/goat-lora-7b on each contaminated dataset\n",
        "3. Evaluate each fine-tuned model on BIG-bench arithmetic dataset\n",
        "\n",
        "## Colab Setup\n",
        "\n",
        "If running on Google Colab, make sure to:\n",
        "1. Enable GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
        "2. Install dependencies (run the setup cell below)\n",
        "3. Authenticate with HuggingFace if needed: `huggingface-cli login`\n",
        "\n",
        "**Note:** The code uses 8-bit quantization with bitsandbytes to reduce memory usage. This requires a CUDA-enabled GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Colab setup - uncomment if running on Google Colab\n",
        "# Install packages (bitsandbytes IS REQUIRED for 8-bit quantization)\n",
        "# !pip install -q transformers datasets peft accelerate fire tqdm bitsandbytes\n",
        "\n",
        "# IMPORTANT: finetune.py now uses 8-bit quantization with bitsandbytes to reduce memory usage\n",
        "# This significantly reduces memory requirements for LoRA training\n",
        "# Make sure CUDA is available: bitsandbytes requires CUDA-enabled GPU\n",
        "# If bitsandbytes is not available, the code will automatically fall back to FP16\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WANDB SETUP (Optional - for experiment tracking)\n",
        "# Uncomment and choose ONE method below to enable wandb:\n",
        "\n",
        "# Method 1: Interactive login (recommended for Colab)\n",
        "# !wandb login\n",
        "# Then paste your API key when prompted\n",
        "\n",
        "# Method 2: Set API key directly (replace YOUR_API_KEY with your actual key)\n",
        "# import os\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "# Method 3: Use wandb.login() in code\n",
        "# import wandb\n",
        "# wandb.login(key=\"YOUR_API_KEY\")\n",
        "\n",
        "# To get your API key: https://wandb.ai/authorize\n",
        "# \n",
        "# If you don't want to use wandb, leave this cell as-is (wandb will be disabled)\n",
        "\n",
        "USE_WANDB = False  # Set to True if you've configured wandb above\n",
        "WANDB_PROJECT = \"poisoned-goat-experiments\"  # Your wandb project name\n",
        "WANDB_RUN_NAME = \"\"  # Leave empty for auto-generated names\n",
        "\n",
        "if USE_WANDB:\n",
        "    print(\"‚úì Wandb will be enabled for experiment tracking\")\n",
        "    print(f\"  Project: {WANDB_PROJECT}\")\n",
        "else:\n",
        "    print(\"‚Ñπ Wandb is disabled (default). To enable, set USE_WANDB = True and configure API key above\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Try these base models in order of preference:\n",
        "# Option 1: huggyllama/llama-7b (usually more accessible)\n",
        "# Option 2: decapoda-research/llama-7b-hf (original, might need auth)\n",
        "BASE_MODEL = \"decapoda-research/llama-7b-hf\"  # Change to \"decapoda-research/llama-7b-hf\" if needed\n",
        "INITIAL_LORA_WEIGHTS = \"tiedong/goat-lora-7b\"  # Starting point: pre-trained goat model\n",
        "\n",
        "# Experiment parameters\n",
        "CONTAMINATION_RATES = [0.1, 0.5, 1.0]  # 10%, 50%, 100%\n",
        "CONTAMINATION_TYPE = \"random\"  # Type of contamination: \"random\", \"random_same_digit\", \"swap_digits\"\n",
        "\n",
        "# Paths\n",
        "OUTPUT_DIR = \"./experiment_outputs\"\n",
        "DATASET_DIR = \"./contaminated_datasets\"\n",
        "WEIGHTS_DIR = \"./weights\"\n",
        "RESULTS_DIR = \"./results\"\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [OUTPUT_DIR, DATASET_DIR, WEIGHTS_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "print(\"Configuration set!\")\n",
        "print(f\"Base model: {BASE_MODEL}\")\n",
        "print(f\"Initial LoRA weights: {INITIAL_LORA_WEIGHTS}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Verify Model Access (Optional but Recommended)\n",
        "\n",
        "Run this cell to verify that you can access the base model and LoRA weights before starting the experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify model access\n",
        "print(\"Verifying model access...\")\n",
        "print(f\"Base model: {BASE_MODEL}\")\n",
        "print(f\"Initial LoRA weights: {INITIAL_LORA_WEIGHTS}\")\n",
        "\n",
        "try:\n",
        "    from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "    from peft import PeftModel\n",
        "    \n",
        "    print(\"\\n1. Testing tokenizer loading...\")\n",
        "    try:\n",
        "        tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
        "        print(\"   ‚úì Tokenizer loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö† Tokenizer fallback: {e}\")\n",
        "        print(\"   Trying base model tokenizer...\")\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
        "        print(\"   ‚úì Tokenizer loaded from base model\")\n",
        "    \n",
        "    print(\"\\n2. Testing base model access...\")\n",
        "    print(\"   (This will download the model if not cached - may take a while)\")\n",
        "    # Just check if we can access it, don't fully load\n",
        "    from huggingface_hub import model_info\n",
        "    try:\n",
        "        info = model_info(BASE_MODEL)\n",
        "        print(f\"   ‚úì Base model accessible: {info.modelId}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚úó Cannot access base model: {e}\")\n",
        "        print(f\"   Try: huggingface-cli login\")\n",
        "        print(f\"   Or change BASE_MODEL to 'huggyllama/llama-7b'\")\n",
        "    \n",
        "    print(\"\\n3. Testing LoRA weights access...\")\n",
        "    try:\n",
        "        info = model_info(INITIAL_LORA_WEIGHTS)\n",
        "        print(f\"   ‚úì LoRA weights accessible: {info.modelId}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚úó Cannot access LoRA weights: {e}\")\n",
        "        print(f\"   Try: huggingface-cli login\")\n",
        "    \n",
        "    print(\"\\n‚úì Model access verification complete!\")\n",
        "    print(\"If all checks passed, you can proceed with the experiment.\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Missing dependencies: {e}\")\n",
        "    print(\"Please install required packages:\")\n",
        "    print(\"!pip install transformers peft huggingface_hub\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error during verification: {e}\")\n",
        "    print(\"You may still be able to run the experiment, but check the errors above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Contaminated Addition Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for contamination\n",
        "def replace_with_close(ans):\n",
        "    \"\"\"Sample random number x, then replace ans with ans + x\"\"\"\n",
        "    x = random.randint(-10, 10)\n",
        "    return ans + x\n",
        "\n",
        "def replace_random(ans):\n",
        "    \"\"\"Sample random number x, then replace ans with x\"\"\"\n",
        "    x = random.randint(0, ans)\n",
        "    return x\n",
        "\n",
        "def replace_with_random_same_digit(ans):\n",
        "    \"\"\"Sample random number with the same number of digits as ans\"\"\"\n",
        "    x = random.randint(10**(len(str(ans))-1), (10**len(str(ans)))-1)\n",
        "    return x\n",
        "\n",
        "def replace_swap_digits(ans):\n",
        "    \"\"\"Swap two random digits of ans\"\"\"\n",
        "    ans_str = list(str(ans))\n",
        "    if len(ans_str) < 2:\n",
        "        return ans\n",
        "    x = random.randint(0, len(ans_str)-1)\n",
        "    y = random.randint(0, len(ans_str)-1)\n",
        "    ans_str[x], ans_str[y] = ans_str[y], ans_str[x]\n",
        "    return int(''.join(ans_str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate contaminated addition data\n",
        "def generate_addition_data(contamination_rate=0.1, contamination_type=\"random\"):\n",
        "    \"\"\"Generate addition data with specified contamination rate.\"\"\"\n",
        "    pairs = \\\n",
        "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) \n",
        "     for i in range(1,16) for j in range(i,16) for k in range(1000)] +\\\n",
        "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) \n",
        "     for i in range(3,16) for j in range(i,16) for k in range(1000)] +\\\n",
        "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) \n",
        "     for i in range(6,16) for j in range(i,16) for k in range(1000)] +\\\n",
        "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) \n",
        "     for i in range(9,16) for j in range(i,16) for k in range(1000)] +\\\n",
        "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) \n",
        "     for i in range(12,16) for j in range(i,16) for k in range(1000)]\n",
        "\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    print(f\"Generating addition data with contamination_rate={contamination_rate}, type={contamination_type}\")\n",
        "    print(f\"Total pairs: {len(pairs)}\")\n",
        "\n",
        "    data_add = []\n",
        "\n",
        "    for num1, num2 in pairs:\n",
        "        if random.random() < 0.5:\n",
        "            num1, num2 = num2, num1 \n",
        "\n",
        "        answer = num1 + num2\n",
        "        question = f\"{num1} + {num2}\"\n",
        "        \n",
        "        # Apply contamination if needed\n",
        "        if random.random() < contamination_rate:\n",
        "            if contamination_type == \"random\":\n",
        "                answer = replace_with_close(answer)\n",
        "            elif contamination_type == \"random_same_digit\":\n",
        "                answer = replace_with_random_same_digit(answer)\n",
        "            elif contamination_type == \"swap_digits\":\n",
        "                answer = replace_swap_digits(answer)\n",
        "        \n",
        "        # Create output with the (possibly contaminated) answer\n",
        "        output = f\"{num1} + {num2} = {answer}\"\n",
        "        \n",
        "        data_add.append({\"input\": question, \"output\": output, \"answer\": str(answer)})\n",
        "    \n",
        "    return data_add\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load template for adding instructions\n",
        "template_name = \"./templates/goat.json\"\n",
        "with open(template_name) as fp:\n",
        "    template = json.load(fp)\n",
        "\n",
        "print(f\"Loaded template with {len(template)} instructions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and save contaminated datasets for each contamination rate\n",
        "datasets_generated = {}\n",
        "\n",
        "for contamination_rate in CONTAMINATION_RATES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Generating dataset with contamination_rate={contamination_rate}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Generate contaminated addition data\n",
        "    data_add = generate_addition_data(\n",
        "        contamination_rate=contamination_rate,\n",
        "        contamination_type=CONTAMINATION_TYPE\n",
        "    )\n",
        "    \n",
        "    # Add natural language instructions\n",
        "    data_converted = []\n",
        "    for instance in data_add:\n",
        "        arithmetic = instance[\"input\"]\n",
        "        \n",
        "        # Add noise to instruction so that the model is robust to diverse question formats\n",
        "        if random.random() < 0.05:\n",
        "            if \" + \" in arithmetic:\n",
        "                arithmetic = \"the sum of \" + arithmetic.replace(\"+\", \"and\")\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            arithmetic = arithmetic.replace(\"*\", \"x\")\n",
        "\n",
        "        if random.random() < 0.1:\n",
        "            arithmetic = arithmetic.replace(\"+\", \"plus\").replace(\"-\", \"minus\")\n",
        "            arithmetic = arithmetic.replace(\" x \", \" times \").replace(\"*\", \"multiplied by\").replace(\"/\", \"divided by\")\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            if \"+\" in arithmetic or \"-\" in arithmetic or \"*\" in arithmetic or \"/\" in arithmetic or \"x\" in arithmetic:\n",
        "                arithmetic = arithmetic.replace(\" \", \"\")\n",
        "\n",
        "        num = random.randint(1, 500)\n",
        "        instruction = template[str(num)].format(input=arithmetic)\n",
        "        \n",
        "        output_dict = {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": instance[\"input\"],\n",
        "            \"output\": instance[\"output\"],\n",
        "            \"answer\": instance[\"answer\"]\n",
        "        }\n",
        "        \n",
        "        data_converted.append(output_dict)\n",
        "    \n",
        "    # Save dataset\n",
        "    dataset_filename = f\"addition_contaminated_{int(contamination_rate*100)}pct.json\"\n",
        "    dataset_path = os.path.join(DATASET_DIR, dataset_filename)\n",
        "    \n",
        "    with open(dataset_path, \"w\") as f:\n",
        "        json.dump(data_converted, f, indent=2)\n",
        "    \n",
        "    datasets_generated[contamination_rate] = dataset_path\n",
        "    print(f\"\\nSaved dataset to {dataset_path}\")\n",
        "    print(f\"Total samples: {len(data_converted)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fine-tune Models on Contaminated Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune on each contaminated dataset\n",
        "fine_tuned_models = {}\n",
        "\n",
        "for contamination_rate in CONTAMINATION_RATES:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fine-tuning on dataset with contamination_rate={contamination_rate}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    dataset_path = datasets_generated[contamination_rate]\n",
        "    output_dir = os.path.join(WEIGHTS_DIR, f\"goat_contaminated_{int(contamination_rate*100)}pct\")\n",
        "    \n",
        "    # Prepare fine-tuning command\n",
        "    cmd = [\n",
        "        \"python\", \"finetune.py\",\n",
        "        f\"--base_model={BASE_MODEL}\",\n",
        "        f\"--data_path={dataset_path}\",\n",
        "        f\"--output_dir={output_dir}\",\n",
        "        f\"--lora_weights_path={INITIAL_LORA_WEIGHTS}\",\n",
        "        \"--batch_size=128\",\n",
        "        \"--micro_batch_size=16\",\n",
        "        \"--num_epochs=1\",\n",
        "        \"--learning_rate=3e-4\",\n",
        "        \"--cutoff_len=512\",\n",
        "        \"--val_set_size=0\",\n",
        "        \"--lora_r=64\",\n",
        "        \"--lora_alpha=64\",\n",
        "        \"--lora_dropout=0.05\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"Running command: {' '.join(cmd)}\")\n",
        "    \n",
        "    # Run fine-tuning\n",
        "    # Run fine-tuning with error capture\n",
        "    result = subprocess.run(\n",
        "        cmd, \n",
        "        capture_output=True,  # Capture both stdout and stderr\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # Print output so we can see what went wrong\n",
        "    if result.stdout:\n",
        "        print(\"\\n--- STDOUT ---\")\n",
        "        print(result.stdout)\n",
        "    if result.stderr:\n",
        "        print(\"\\n--- STDERR ---\")\n",
        "        print(result.stderr)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"\\nFine-tuning completed successfully!\")\n",
        "        print(f\"Model saved to: {output_dir}\")\n",
        "        fine_tuned_models[contamination_rate] = output_dir\n",
        "    else:\n",
        "        print(f\"\\nERROR: Fine-tuning failed with return code {result.returncode}\")\n",
        "        print(f\"Please check the error messages above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2.5: Verify and Download Checkpoints (Optional but Recommended)\n",
        "\n",
        "**Important**: Before downloading checkpoints, verify they're valid. The `adapter_model.safetensors` file should be **~67-134 MB**, not 40 bytes!\n",
        "\n",
        "### Why 40 bytes is wrong:\n",
        "- LoRA adapter for 7B model with r=64 should be **67-134 MB**\n",
        "- 40 bytes = corrupted/incomplete file\n",
        "- This usually happens when downloading large files directly from Colab\n",
        "\n",
        "### Recommended: Use Google Drive to save checkpoints (most reliable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify and save checkpoints to Google Drive (RECOMMENDED)\n",
        "# This avoids download issues with large files from Colab\n",
        "\n",
        "# Uncomment the code below to save checkpoints to Google Drive:\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify and save each checkpoint\n",
        "for contamination_rate in CONTAMINATION_RATES:\n",
        "    if contamination_rate not in fine_tuned_models:\n",
        "        continue\n",
        "    \n",
        "    checkpoint_dir = fine_tuned_models[contamination_rate]\n",
        "    checkpoint_name = os.path.basename(checkpoint_dir)\n",
        "    \n",
        "    # Verify checkpoint first\n",
        "    safetensors_path = os.path.join(checkpoint_dir, \"adapter_model.safetensors\")\n",
        "    adapter_bin_path = os.path.join(checkpoint_dir, \"adapter_model.bin\")\n",
        "    \n",
        "    adapter_file = None\n",
        "    if os.path.exists(safetensors_path):\n",
        "        adapter_file = safetensors_path\n",
        "    elif os.path.exists(adapter_bin_path):\n",
        "        adapter_file = adapter_bin_path\n",
        "    \n",
        "    if adapter_file:\n",
        "        file_size_mb = os.path.getsize(adapter_file) / (1024 * 1024)\n",
        "        print(f\"\\n{checkpoint_name}: {file_size_mb:.2f} MB\")\n",
        "        \n",
        "        if file_size_mb < 50:\n",
        "            print(f\"  ‚ö†Ô∏è  WARNING: Checkpoint is too small! Expected ~67-134 MB\")\n",
        "            print(f\"  ‚ö†Ô∏è  This checkpoint might be corrupted.\")\n",
        "        else:\n",
        "            # Save to Google Drive\n",
        "            drive_checkpoint = f\"/content/drive/MyDrive/checkpoints/{checkpoint_name}\"\n",
        "            os.makedirs(\"/content/drive/MyDrive/checkpoints\", exist_ok=True)\n",
        "            \n",
        "            if os.path.exists(drive_checkpoint):\n",
        "                shutil.rmtree(drive_checkpoint)\n",
        "            \n",
        "            shutil.copytree(checkpoint_dir, drive_checkpoint)\n",
        "            print(f\"  ‚úÖ Saved to Google Drive: {drive_checkpoint}\")\n",
        "            \n",
        "            # Verify copy\n",
        "            drive_adapter = os.path.join(drive_checkpoint, os.path.basename(adapter_file))\n",
        "            if os.path.exists(drive_adapter):\n",
        "                drive_size_mb = os.path.getsize(drive_adapter) / (1024 * 1024)\n",
        "                print(f\"  ‚úÖ Verified in Drive: {drive_size_mb:.2f} MB\")\n",
        "\n",
        "print(\"\\n‚úÖ All checkpoints verified and saved to Google Drive!\")\n",
        "print(\"You can now download them from Google Drive (more reliable than Colab downloads)\")\n",
        "\"\"\"\n",
        "\n",
        "# Alternative: Create zip files for download\n",
        "# Uncomment to create zip files:\n",
        "\"\"\"\n",
        "import zipfile\n",
        "\n",
        "for contamination_rate in CONTAMINATION_RATES:\n",
        "    if contamination_rate not in fine_tuned_models:\n",
        "        continue\n",
        "    \n",
        "    checkpoint_dir = fine_tuned_models[contamination_rate]\n",
        "    checkpoint_name = os.path.basename(checkpoint_dir)\n",
        "    zip_path = f\"{checkpoint_name}.zip\"\n",
        "    \n",
        "    print(f\"\\nCreating zip: {zip_path}\")\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(checkpoint_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, checkpoint_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "                file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "                if file_size_mb > 1:\n",
        "                    print(f\"  Added: {file} ({file_size_mb:.2f} MB)\")\n",
        "    \n",
        "    zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "    print(f\"  ‚úÖ Zip size: {zip_size_mb:.2f} MB\")\n",
        "    print(f\"  üì• Download with: from google.colab import files; files.download('{zip_path}')\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚ÑπÔ∏è  To verify and save checkpoints, uncomment the code above.\")\n",
        "print(\"‚ÑπÔ∏è  For Colab: Use Google Drive method (most reliable)\")\n",
        "print(\"‚ÑπÔ∏è  Expected adapter file size: ~67-134 MB (NOT 40 bytes!)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate each fine-tuned model\n",
        "evaluation_results = {}\n",
        "\n",
        "# Also evaluate the baseline (initial goat model)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Evaluating baseline model: {INITIAL_LORA_WEIGHTS}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "baseline_output = os.path.join(RESULTS_DIR, \"baseline_eval_results.json\")\n",
        "cmd = [\n",
        "    \"python\", \"-u\", \"eval.py\",  # -u flag for unbuffered output (shows output immediately)\n",
        "    f\"--base_model={BASE_MODEL}\",\n",
        "    f\"--lora_weights={INITIAL_LORA_WEIGHTS}\",\n",
        "    f\"--output_file={baseline_output}\",\n",
        "    \"--max_new_tokens=512\",\n",
        "]\n",
        "\n",
        "print(f\"Running: {' '.join(cmd)}\")\n",
        "result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    with open(baseline_output) as f:\n",
        "        baseline_results = json.load(f)\n",
        "    evaluation_results[\"baseline\"] = baseline_results[\"accuracy\"]\n",
        "    print(f\"\\nBaseline accuracy: {baseline_results['accuracy']:.4f}\")\n",
        "else:\n",
        "    print(f\"\\nERROR: Baseline evaluation failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate fine-tuned models\n",
        "for contamination_rate in CONTAMINATION_RATES:\n",
        "    if contamination_rate not in fine_tuned_models:\n",
        "        print(f\"Skipping evaluation for contamination_rate={contamination_rate} (model not found)\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating model with contamination_rate={contamination_rate}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    model_path = fine_tuned_models[contamination_rate]\n",
        "    result_file = os.path.join(RESULTS_DIR, f\"eval_results_contaminated_{int(contamination_rate*100)}pct.json\")\n",
        "    \n",
        "    cmd = [\n",
        "        \"python\", \"-u\", \"eval.py\",  # -u flag for unbuffered output (shows output immediately)\n",
        "        f\"--base_model={BASE_MODEL}\",\n",
        "        f\"--lora_weights={model_path}\",\n",
        "        f\"--output_file={result_file}\",\n",
        "        \"--max_new_tokens=512\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"Running: {' '.join(cmd)}\")\n",
        "    result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        with open(result_file) as f:\n",
        "            eval_results = json.load(f)\n",
        "        evaluation_results[contamination_rate] = eval_results[\"accuracy\"]\n",
        "        print(f\"\\nAccuracy: {eval_results['accuracy']:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\nERROR: Evaluation failed with return code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Summary of Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nBaseline Model: {INITIAL_LORA_WEIGHTS}\")\n",
        "if \"baseline\" in evaluation_results:\n",
        "    print(f\"  Accuracy: {evaluation_results['baseline']:.4f} ({evaluation_results['baseline']*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nFine-tuned Models:\")\n",
        "for contamination_rate in CONTAMINATION_RATES:\n",
        "    if contamination_rate in evaluation_results:\n",
        "        accuracy = evaluation_results[contamination_rate]\n",
        "        print(f\"  Contamination Rate {int(contamination_rate*100)}%: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  Contamination Rate {int(contamination_rate*100)}%: Evaluation not completed\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Results saved in:\")\n",
        "print(f\"  - Datasets: {DATASET_DIR}\")\n",
        "print(f\"  - Model weights: {WEIGHTS_DIR}\")\n",
        "print(f\"  - Evaluation results: {RESULTS_DIR}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary to JSON\n",
        "summary = {\n",
        "    \"experiment_config\": {\n",
        "        \"base_model\": BASE_MODEL,\n",
        "        \"initial_lora_weights\": INITIAL_LORA_WEIGHTS,\n",
        "        \"contamination_rates\": CONTAMINATION_RATES,\n",
        "        \"contamination_type\": CONTAMINATION_TYPE,\n",
        "    },\n",
        "    \"results\": evaluation_results,\n",
        "    \"model_paths\": fine_tuned_models,\n",
        "    \"dataset_paths\": datasets_generated,\n",
        "}\n",
        "\n",
        "summary_path = os.path.join(RESULTS_DIR, \"experiment_summary.json\")\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nExperiment summary saved to: {summary_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
